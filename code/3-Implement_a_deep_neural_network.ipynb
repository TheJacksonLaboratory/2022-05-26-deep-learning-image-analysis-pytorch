{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf16a8ca-06d0-4970-a62e-43896aa5d3e1",
   "metadata": {},
   "source": [
    "# **Deep learning for image analysis with Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1395d22-9bc4-4820-a8cf-02ef76875f4b",
   "metadata": {},
   "source": [
    "#### Fernando Cervantes, Systems Analyst I, Imaging Solutions, Research IT\n",
    "#### fernando.cervantes@jax.org    (slack) @fernando.cervantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84014c2-b86a-4161-914c-fd68d5a4907d",
   "metadata": {},
   "source": [
    "## **3 Implement a deep neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2243ddf-0100-4636-a222-51a2eb4ce662",
   "metadata": {},
   "source": [
    "## 3.1 _Neural network modules_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d689fcf-b553-4d59-a016-0b4cb84eb71e",
   "metadata": {},
   "source": [
    "PyTorch provides several operations that can be used as building blocks to construct a neural network.<br>\n",
    "Each operation is commonly referred as a **Module**, and those are implemented inside the *nn* module of pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec8ff94-64f8-4f2e-b848-affdffde4198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77370a20-288f-421a-bf58-67476c28ec0a",
   "metadata": {},
   "source": [
    "![Image](https://pytorch.org/tutorials/_images/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02906423-1b69-4b10-8acf-429ce5f8cb3e",
   "metadata": {},
   "source": [
    "Lets create the first convolutional layer from the LeNet architecture.<br>\n",
    "That layer applies **six** $5\\times5$ linear kernels over every pixel of the input image.<br>\n",
    "More details of the available parameters to create a two dimensional convolutional layer can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d06be443-d2f5-4039-8e85-e0fe3d759639",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_1 = nn.Conv2d(\n",
    "    in_channels=1, # Because the input image is in gray-levels\n",
    "    out_channels=6, # to generate six new feature maps / channels\n",
    "    kernel_size=5,\n",
    "    stride=1, # to pass the kernel filters over each pixel of the image\n",
    "    padding=0, # do not add padding to the image edges (this will reduce the size of the output)\n",
    "    bias=False # do not add a bias intercept to the output of this layer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb4527c5-ff4d-4d42-af7e-887c87c70a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75b41b83-824c-4a3d-a98d-a2e53fdf6b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1507, -0.0824, -0.1526, -0.0141, -0.0359],\n",
      "          [-0.1659, -0.0180, -0.0448, -0.1615,  0.0865],\n",
      "          [ 0.1426,  0.1702,  0.1123,  0.1452,  0.1622],\n",
      "          [ 0.1539, -0.1971, -0.1505, -0.0166, -0.0601],\n",
      "          [ 0.0725, -0.1070, -0.1634, -0.0151,  0.0615]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1305,  0.0272, -0.0253,  0.1500,  0.1329],\n",
      "          [ 0.1118, -0.0529,  0.1432,  0.0574,  0.1701],\n",
      "          [-0.1109,  0.0641,  0.1951,  0.1996,  0.1766],\n",
      "          [ 0.0248, -0.1185, -0.1578,  0.0746,  0.0195],\n",
      "          [ 0.1953, -0.0111,  0.1623,  0.0767,  0.1769]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0219, -0.1884,  0.1405, -0.0339,  0.0444],\n",
      "          [-0.0802, -0.1462, -0.1162, -0.1651, -0.0350],\n",
      "          [ 0.1818, -0.1035, -0.1020,  0.1539,  0.1052],\n",
      "          [ 0.0421,  0.1311,  0.1847,  0.1370, -0.0482],\n",
      "          [ 0.1828,  0.1060,  0.1709,  0.0451,  0.0193]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0563,  0.0274, -0.1125,  0.1380,  0.1097],\n",
      "          [-0.1477, -0.0965, -0.0036, -0.0332, -0.0800],\n",
      "          [ 0.0020, -0.1821, -0.1535, -0.0028, -0.0168],\n",
      "          [-0.1093,  0.0460, -0.1905, -0.0808,  0.1881],\n",
      "          [-0.0405,  0.0019, -0.0525, -0.0887, -0.0989]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0134, -0.0291, -0.1393,  0.1951, -0.1402],\n",
      "          [-0.0856, -0.0937,  0.0718, -0.1564,  0.1528],\n",
      "          [-0.0176,  0.0458, -0.1581,  0.0550, -0.1217],\n",
      "          [-0.0012,  0.1280,  0.1388, -0.1662, -0.0308],\n",
      "          [-0.0672,  0.0978, -0.1167, -0.0191, -0.1645]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0105,  0.1168,  0.0530,  0.0546, -0.0451],\n",
      "          [ 0.1768,  0.0727, -0.1774, -0.0952,  0.0217],\n",
      "          [ 0.0770,  0.0028,  0.0865, -0.1884,  0.0345],\n",
      "          [-0.0786,  0.0791,  0.0963,  0.0068,  0.0751],\n",
      "          [-0.0292,  0.0192, -0.1809,  0.1238,  0.1564]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(conv_1.weight.size())\n",
    "print(conv_1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb59be9-526f-4d62-be37-d0a4ae3fcf56",
   "metadata": {},
   "source": [
    "Because we defined *conv_1* using a **nn.Module**, the operation will track the gradients for the weights and bias of that operation.<br>\n",
    "Those weights and bias are kwnown as the *learnable parameters*.<br>\n",
    "By default, bias is always added to linear and convolution operations.<br>\n",
    "This single layer has $6\\times1\\times5\\times 5 = 150$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a925e5-eeb6-4e80-9984-6e11bd7663be",
   "metadata": {},
   "source": [
    "***\n",
    "The following operation applied to the first convolution layer is a ReLU activation function.<br>\n",
    "These functions can be found also inside the PyTorch's *nn* module (follow this [link](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) to see the different activation functions available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "353e7fcc-8472-4912-b792-29a0fcb6e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_1 = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cefc16e-6375-412b-8b75-0d8870e34840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a58586-7d34-4033-a27e-f295e4e121fd",
   "metadata": {},
   "source": [
    "***\n",
    "The subsampling operation (S2 in the LeNet's illustration) is implemented by a *maximum pooling* operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f10011d1-d9e5-4a8f-8334-a547a036c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_1 = nn.MaxPool2d(\n",
    "    kernel_size=2,\n",
    "    stride=2,\n",
    "    padding=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7583265d-13e1-43d0-b3e8-71ea483aead5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ff9be-3ebf-48f5-9ff7-240c7cd5563b",
   "metadata": {},
   "source": [
    "***\n",
    "Depending the task, the last layers of a network will be used to abstract the spatial information into a one-dimensional representation.\n",
    "This is achieved by *flattening* the tensor into a *vector*, to perform linear operations (matrix-vector).\n",
    "These layers are known as **Fully Connected** (FC), and are implemented by multilayer perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f5a1093-553d-4641-aa27-85bf59c0a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_1 = nn.Linear(\n",
    "    in_features=16*5*5, \n",
    "    out_features=120, \n",
    "    bias=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81047adc-202a-4365-a353-a5cbcfcb6e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=400, out_features=120, bias=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b57214-061c-4e4c-b984-ea9315c1d869",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.2 _Neural network inputs_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e0b12-a5ab-4e0d-a140-1693341fa456",
   "metadata": {},
   "source": [
    "PyTorch modules expect the inputs to be in the shape of *batch size* $\\times$ *input channels* $\\times$ *height* $\\times$ *width* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3654155-b466-43ae-87c0-fc261149ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1, 1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad30ca-5235-4eb5-8055-a7f925da1cfa",
   "metadata": {},
   "source": [
    "Modules defined from the *nn* module have a built-in *forward* function.\n",
    "The default behavior of nn modules when called is to use their correspondig *forward* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9b22d2c-119a-47e8-a75e-d0db36418c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = conv_1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d51deec7-5da5-441e-aa33-ba7ccce19488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 28, 28])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55c8cd7-c9ef-4012-93a1-7b02df3cdc38",
   "metadata": {},
   "source": [
    "In the LeNet's architecture illustration, the output of the first convolution layer (C1) are six feature maps of size $28\\times28$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cce7c5-9ddf-4a7c-a226-0add83daf1ec",
   "metadata": {},
   "source": [
    "![Image](https://pytorch.org/tutorials/_images/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47743f0-aee6-4be2-aa15-293059b3aae2",
   "metadata": {},
   "source": [
    "***\n",
    "The next operation is a ReLU activation function that is applied element-wise to each element of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2144fe1-7694-44a6-992e-c092a89fe272",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = act_1(fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6569a144-2180-4fcb-a414-5f957886f9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 28, 28])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbd970c-c1c4-4485-9d97-b1c9edffbeeb",
   "metadata": {},
   "source": [
    "***\n",
    "The subsampling operation is applied using a kernel of size $2\\times2$, and is applied every $2$ pixels, resulting on a feature map with half the size of the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffdff4e8-b7d9-43d2-b755-8a17db9ca3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = sub_1(fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb73bb2c-5f3b-4c78-86b9-c1a6b2bff139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 14, 14])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f108d2-2402-4c2c-96d7-d213fb1e40ad",
   "metadata": {},
   "source": [
    "Now, the feature maps have a size of $14\\times14$, just as illustrated in the LeNet's architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705b2cc2-f89f-4822-a954-6be9abd9eb5c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fef627-1a22-41e7-8567-c2270f309af2",
   "metadata": {},
   "source": [
    "## 3.3 _Defining a neural network architecture as a python class_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fb875e-77dc-401b-801b-bca2447523fa",
   "metadata": {},
   "source": [
    "PyTorch provides a *pythonic* framework to develop neural networks.\n",
    "For that reason, architectures are defined as classes derived from the **nn.Module** class.<br>\n",
    "A class defining an neural network architecture is rquired to call the nn.Module initialization function and implement a **forward** function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9104e6a-09f1-4f5b-b30c-24b77c0a96b6",
   "metadata": {},
   "source": [
    "![Image](https://pytorch.org/tutorials/_images/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b5f9d-77fc-47d3-bd68-24dd2f84fe3d",
   "metadata": {},
   "source": [
    "Fully connected layers are a fancy name for Multilayer perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2af2a8a-62eb-4648-83cf-e78f2e1ad7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        \"\"\"\n",
    "        Always call the initialization function from the nn.Module parent class.\n",
    "        This way all parameters from the operations defined as members of *this* class are tracked for their optimization.\n",
    "        \"\"\"\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels=in_channels, out_channels=6, kernel_size=5, stride=1, padding=0, bias=False)\n",
    "        self.act_1 = nn.ReLU()\n",
    "        self.sub_1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=False)\n",
    "        self.act_2 = nn.ReLU()\n",
    "        self.sub_2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(in_features=5*5*16, out_features=120, bias=True)\n",
    "        self.act_fc_1 = nn.ReLU()\n",
    "        \n",
    "        self.fc_2 = nn.Linear(in_features=120, out_features=84, bias=True)\n",
    "        self.act_fc_2 = nn.ReLU()\n",
    "        \n",
    "        self.fc_3 = nn.Linear(in_features=84, out_features=num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution layers to extract feature maps with image context\n",
    "        fx = self.act_1(self.conv_1(x))\n",
    "        fx = self.sub_1(fx)\n",
    "        \n",
    "        fx = self.act_2(self.conv_2(fx))\n",
    "        fx = self.sub_2(fx)\n",
    "        \n",
    "        # Flatten the feature maps to perform linear operations\n",
    "        fx = fx.view(-1, 16*5*5)\n",
    "        \n",
    "        fx = self.act_fc_1(self.fc_1(fx))\n",
    "        fx = self.act_fc_2(self.fc_2(fx))\n",
    "        fx = self.fc_3(fx)\n",
    "        \n",
    "        y = torch.softmax(fx, dim=1)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26700d58-e5c5-4992-8c80-5bb5a5b1e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f4e7084-a440-4981-876a-fa4de22dc89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv_1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "  (act_1): ReLU()\n",
      "  (sub_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv_2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "  (act_2): ReLU()\n",
      "  (sub_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc_1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (act_fc_1): ReLU()\n",
      "  (fc_2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (act_fc_2): ReLU()\n",
      "  (fc_3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae7421ef-5431-44e8-8679-b20c82920c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b95f74f-c373-484d-beb2-7e5b66eb11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa6bc78c-090d-4803-9d14-ecf52fe5a3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5071be95-3d16-406f-b59b-6c5254d8727d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1055, 0.1062, 0.0923, 0.1089, 0.0953, 0.1045, 0.0970, 0.0953, 0.0931,\n",
      "         0.1020]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b53c8295-1375-4c6e-a49b-140299e8aee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(y.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db3f9514-0bbd-42c6-947f-2ac0246055d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for par in net.parameters():\n",
    "    total_parameters += torch.numel(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b2fdd40-8f93-400b-bc7d-273035354c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61684"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c64ca7-0504-41e2-9104-3991fb0b2de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
