{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a58529-c823-4fb2-9b31-1e9ac27523af",
   "metadata": {},
   "source": [
    "# **Deep learning for image analysis with PyTorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785b9c34-4741-4b49-8646-76c29a51898f",
   "metadata": {},
   "source": [
    "#### Fernando Cervantes, Systems Analyst I, Imaging Solutions, Research IT\n",
    "#### fernando.cervantes@jax.org    (slack) @fernando.cervantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e2e073-d95e-43f7-891d-c7bab241c2d0",
   "metadata": {},
   "source": [
    "## **4 Define the optimization problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18aeaef-dc6b-467d-9318-7212fe6ddad9",
   "metadata": {},
   "source": [
    "The performance of an artificial neural network depends on the architecture of the network and the value of its parameters $\\theta$.<br>\n",
    "These paramateres are fitted (optimized) through a process known as *training*.<br>\n",
    "The *training* process is defined as an optimization problem which target is to minimize a loss function $L$.<br>\n",
    "For tasks where a set of examples ($X$) and their expected output/ground-truth ($Y$) are available the fitting process is known as *supervised training*.<br>\n",
    "On those cases, the target loss is the *Training Error* which is computed as the average loss on the training examples as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69773f-1857-4a4d-afac-c9726025a834",
   "metadata": {},
   "source": [
    "$\\hat{\\theta} = \\text{arg}\\,\\min\\limits_{\\theta}\\, L(X, Y, \\theta) = \\text{arg}\\,\\min\\limits_{\\theta}\\, \\frac{1}{N}\\sum\\limits_{i=1}^{N}L(x_i, y_i, \\theta)$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718033e7-e21d-4009-940b-8d100b8835c9",
   "metadata": {},
   "source": [
    "### 4.1 _Loss functions_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3afc66-bc6a-40c3-b72f-3c0d2f9be0fa",
   "metadata": {},
   "source": [
    "The two most common tasks for neural networks are regression and classification.<br>\n",
    "Regression consists of the prediction of a real valued target, while classification is the prediction of a category target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5dcd2-98a7-4133-9088-0ec12fb2fb3f",
   "metadata": {},
   "source": [
    "Regression error $L(x, y, \\theta) = \\left(y - f_\\theta(x)\\right)^2$ [*Mean Squared Error (MSE)*](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html?highlight=mse#torch.nn.MSELoss)<br>\n",
    "Classification error $L(x, y, \\theta) = -\\sum\\limits_{g=1}^G y_g \\log\\left(f_{\\theta,g}(x)\\right)$ [*Cross-Entropy (CE)*](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=cross%20entropy#torch.nn.CrossEntropyLoss)/*Kullbackâ€“Leibler divergence*<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e783321-3093-4225-a4fa-fd1f89d00fdf",
   "metadata": {},
   "source": [
    "Special case of classification of two categories,  $L(x, y, \\theta) = -y \\log\\left(f_\\theta(x)\\right) - (1-y) \\log\\left(1-f_\\theta(x)\\right)$ [*Binary Cross-Entropy (BCE)*](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html?highlight=binary%20cross%20entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c47f34b-5091-41c0-89c2-8b15ca534a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4858bb-3c73-44cb-9fec-38f30fb36033",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss() # Mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3237320-f087-4bdd-a2ae-451ba834dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "157d56b5-d687-46a7-a7db-04c5cffefc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss() # Binary Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ff3db-230c-413a-9329-f16f1e8d4c07",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### 4.2 _Validation loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead69248-d5c1-44f3-9fb5-228797a5a440",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "The *training error* is not allways a good estimation of the *test error*.<br>\n",
    "For that reason, training a neural network is perfomed under a cross-validation scheme.<br>\n",
    "In these schemes, the *validation error* is used to estimate the *test error* since the *validation examples* are not used to train the model.<br>\n",
    "Then, the best model is chosen as the one that minimizes the *validation error*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71fb2c8-a0f2-4802-b30a-11c567c3bf54",
   "metadata": {},
   "source": [
    "### 4.3 _Optimization algorithms_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65bf76e-e32b-42e5-9d07-577730401c6a",
   "metadata": {},
   "source": [
    "*Gradient-based* optimization algorithms have been the preferred ones to fit neural network's parameters.<br>\n",
    "*Gradient descent* (GD) is one of the simplest gradient-based algotithms, and is defined as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a65396-e4d2-46c2-9a83-74cf79ac9082",
   "metadata": {},
   "source": [
    "$\\theta^{t+1} = \\theta^t - \\alpha \\nabla L(X, Y, \\theta^t)$.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18000bba-5c2b-49c0-acd1-2de64585678c",
   "metadata": {},
   "source": [
    "***\n",
    "The *Stochastic Gradient Descent* (SGD) allows to train the network parameters *on-line* for large datasets.<br>\n",
    "This means that the optimization occurs after processing an individual batch instead of waiting until all training examples have been processed.<br>\n",
    "This method updates the parameters of the model as follows<br>\n",
    "$\\theta^{t+1} = \\theta^t - \\alpha \\nabla L(\\{(x_i, y_i)\\}_i^B, \\theta^t)$,<br>\n",
    "where <br>\n",
    "- $\\{(x_i, y_i)\\}_i^B$ is a mini batch of training examples and their corresponding ground-truth, <br>\n",
    "- $\\nabla L(\\{(x_i, y_i)\\}_i^B, \\theta)$ is the gradient computed for the current $\\theta$ at time $t$, and <br>\n",
    "- $\\alpha$ is the step size (*learning rate*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be2bbc-c201-44af-ae64-cf1939b82bf6",
   "metadata": {},
   "source": [
    "***\n",
    "#### The difference between SGD and GD\n",
    "In SGD ocurrs after each mini-batch, and in GD the update is computed after all examples have been processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d5b7b-f4ef-4a7a-8c99-18941229ec8d",
   "metadata": {},
   "source": [
    "***\n",
    "In PyTorch, there a wide variety of [optimizers](https://pytorch.org/docs/stable/optim.html) including [*SGD*](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) and [*ADAM*](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam).<br>\n",
    "All those optimizers are found in the *torch.optim* module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "1cbf3e25-122e-43ea-8d7c-309502dcc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "22934a4a-c2ac-4105-bdfa-3b82cc7de7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.sgd.SGD"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "8900016d-2570-4601-ba96-10bc245969be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.adam.Adam"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim.Adam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
