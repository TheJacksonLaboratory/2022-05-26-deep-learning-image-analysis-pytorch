{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "177db554-5724-456f-9889-e63382b7c40a",
   "metadata": {},
   "source": [
    "# **Deep learning for image analysis with Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34661982-88ba-41d0-b84d-04f47dbd478b",
   "metadata": {},
   "source": [
    "#### Fernando Cervantes, Systems Analyst I, Imaging Solutions, Research IT\n",
    "#### fernando.cervantes@jax.org    (slack) @fernando.cervantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dfa69a-253b-4de8-b057-8c6fe131ffed",
   "metadata": {},
   "source": [
    "## 6 Monitoring and logging the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b85f28-6b38-4698-adae-f4c2fe4fa8da",
   "metadata": {},
   "source": [
    "It is important to track the training process. By doing that, we can detect interesting behavior of our network, possible failures, and even *overfitting*.<br>\n",
    "This also helps to save the results of different experiments performed using distinct configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c55f70-f389-4082-a1d3-0e14262a5310",
   "metadata": {},
   "source": [
    "### 6.1 _Logging the network performance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d2abb-69d6-41da-8fcc-aee4d199d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "cifar_data = CIFAR100(root=r'C:\\Users\\cervaf\\Documents\\Datasets',\n",
    "                             download=False,\n",
    "                             train=True,\n",
    "                             transform=ToTensor()\n",
    "                            )\n",
    "\n",
    "cifar_loader = DataLoader(cifar_data,\n",
    "                              batch_size=128,\n",
    "                              shuffle=True,\n",
    "                              pin_memory=True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a6ab29-f018-40c6-bdfc-5bfad25baaff",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 46>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m net \u001b[38;5;241m=\u001b[39m LeNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     44\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 46\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m criterion\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\cnn_comp\\lib\\site-packages\\torch\\nn\\modules\\module.py:680\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \n\u001b[0;32m    666\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\cnn_comp\\lib\\site-packages\\torch\\nn\\modules\\module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\cnn_comp\\lib\\site-packages\\torch\\nn\\modules\\module.py:593\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 593\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\cnn_comp\\lib\\site-packages\\torch\\nn\\modules\\module.py:680\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \n\u001b[0;32m    666\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\cnn_comp\\lib\\site-packages\\torch\\cuda\\__init__.py:208\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        \"\"\"\n",
    "        Always call the initialization function from the nn.Module parent class.\n",
    "        This way all parameters from the operations defined as members of *this* class are tracked for their optimization.\n",
    "        \"\"\"\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels=in_channels, out_channels=6, kernel_size=5)\n",
    "        self.sub_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.sub_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(in_features=5*5*16, out_features=120)\n",
    "        self.fc_2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc_3 = nn.Linear(in_features=84, out_features=num_classes)\n",
    "        \n",
    "        self.act_fn = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution layers to extract feature maps with image context\n",
    "        fx = self.act_fn(self.conv_1(x))\n",
    "        fx = self.sub_1(fx)\n",
    "        \n",
    "        fx = self.act_fn(self.conv_2(fx))\n",
    "        fx = self.sub_2(fx)\n",
    "        \n",
    "        # Flatten the feature maps to perform linear operations\n",
    "        fx = fx.view(-1, 16*5*5)\n",
    "        \n",
    "        fx = self.act_fn(self.fc_1(fx))\n",
    "        fx = self.act_fn(self.fc_2(fx))\n",
    "        y = self.fc_3(fx)\n",
    "        \n",
    "        return y\n",
    "\n",
    "net = LeNet(in_channels=3, num_classes=100)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "net.cuda()\n",
    "criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583c78f4-f39e-41e8-9618-b7c9c9f59459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    params=net.parameters(),\n",
    "    lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f5b23-be32-4296-9285-3df0859fe2d3",
   "metadata": {},
   "source": [
    "***\n",
    "Now that we have set up our experiment, lets create a summary writer for our training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9625d70c-4254-4731-b1f3-68d1a7414ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9495c77e-aa33-4047-90be-90fb35aae8ec",
   "metadata": {},
   "source": [
    "Create a summary writter using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e944094a-9d99-4429-987a-2a089cecaf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/LR_0.001_BATCH_128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b67bcb7f-ce89-482f-9ed8-eba427474154",
   "metadata": {},
   "outputs": [],
   "source": [
    "starter = torch.cuda.Event(enable_timing=True)\n",
    "ender = torch.cuda.Event(enable_timing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9367f0c3-b1ef-4a8e-874b-a50c4deee927",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 00] 3.65062169\n",
      "[Epoch 01] 2.88800209\n",
      "[Epoch 02] 2.36192711\n",
      "[Epoch 03] 1.96028529\n",
      "[Epoch 04] 1.59957976\n",
      "[Epoch 05] 1.22611987\n",
      "[Epoch 06] 0.87889902\n",
      "[Epoch 07] 0.54227505\n",
      "[Epoch 08] 0.33578738\n",
      "[Epoch 09] 0.21201504\n",
      "[Epoch 10] 0.20038928\n",
      "[Epoch 11] 0.17722930\n",
      "[Epoch 12] 0.15801338\n",
      "[Epoch 13] 0.14833670\n",
      "[Epoch 14] 0.12397254\n",
      "[Epoch 15] 0.10777072\n",
      "[Epoch 16] 0.09825277\n",
      "[Epoch 17] 0.13012789\n",
      "[Epoch 18] 0.11447810\n",
      "[Epoch 19] 0.08842344\n",
      "[Epoch 20] 0.07526941\n",
      "[Epoch 21] 0.08866941\n",
      "[Epoch 22] 0.09730977\n",
      "[Epoch 23] 0.09207045\n",
      "[Epoch 24] 0.07990088\n",
      "[Epoch 25] 0.06634067\n",
      "[Epoch 26] 0.07914605\n",
      "[Epoch 27] 0.06461058\n",
      "[Epoch 28] 0.05971114\n",
      "[Epoch 29] 0.07999803\n",
      "[Epoch 30] 0.06231711\n",
      "[Epoch 31] 0.05414208\n",
      "[Epoch 32] 0.05418844\n",
      "[Epoch 33] 0.06799812\n",
      "[Epoch 34] 0.07609604\n",
      "[Epoch 35] 0.05467035\n",
      "[Epoch 36] 0.04762390\n",
      "[Epoch 37] 0.05371990\n",
      "[Epoch 38] 0.05767364\n",
      "[Epoch 39] 0.05375689\n",
      "[Epoch 40] 0.04962738\n",
      "[Epoch 41] 0.04518812\n",
      "[Epoch 42] 0.04953536\n",
      "[Epoch 43] 0.04910351\n",
      "[Epoch 44] 0.03543905\n",
      "[Epoch 45] 0.04314062\n",
      "[Epoch 46] 0.05171530\n",
      "[Epoch 47] 0.05096803\n",
      "[Epoch 48] 0.04794429\n",
      "[Epoch 49] 0.03370862\n",
      "[Epoch 50] 0.03451495\n",
      "[Epoch 51] 0.04013908\n",
      "[Epoch 52] 0.03925790\n",
      "[Epoch 53] 0.04407114\n",
      "[Epoch 54] 0.04067117\n",
      "[Epoch 55] 0.03953718\n",
      "[Epoch 56] 0.03169098\n",
      "[Epoch 57] 0.03132177\n",
      "[Epoch 58] 0.03371198\n",
      "[Epoch 59] 0.03776361\n",
      "[Epoch 60] 0.03959062\n",
      "[Epoch 61] 0.03376758\n",
      "[Epoch 62] 0.02652608\n",
      "[Epoch 63] 0.02620476\n",
      "[Epoch 64] 0.03458314\n",
      "[Epoch 65] 0.04330012\n",
      "[Epoch 66] 0.03795301\n",
      "[Epoch 67] 0.02774782\n",
      "[Epoch 68] 0.02349931\n",
      "[Epoch 69] 0.02845003\n",
      "[Epoch 70] 0.03517196\n",
      "[Epoch 71] 0.03546220\n",
      "[Epoch 72] 0.02880258\n",
      "[Epoch 73] 0.02689426\n",
      "[Epoch 74] 0.02865353\n",
      "[Epoch 75] 0.02268432\n",
      "[Epoch 76] 0.03183872\n",
      "[Epoch 77] 0.03670000\n",
      "[Epoch 78] 0.02411187\n",
      "[Epoch 79] 0.02367042\n",
      "[Epoch 80] 0.02023517\n",
      "[Epoch 81] 0.02761166\n",
      "[Epoch 82] 0.03043887\n",
      "[Epoch 83] 0.02376364\n",
      "[Epoch 84] 0.02586239\n",
      "[Epoch 85] 0.02358760\n",
      "[Epoch 86] 0.02071400\n",
      "[Epoch 87] 0.02107355\n",
      "[Epoch 88] 0.02448655\n",
      "[Epoch 89] 0.02208858\n",
      "[Epoch 90] 0.02601843\n",
      "[Epoch 91] 0.02710170\n",
      "[Epoch 92] 0.02898848\n",
      "[Epoch 93] 0.01968700\n",
      "[Epoch 94] 0.01625939\n",
      "[Epoch 95] 0.01865933\n",
      "[Epoch 96] 0.02371779\n",
      "[Epoch 97] 0.03498227\n",
      "[Epoch 98] 0.01883862\n",
      "[Epoch 99] 0.01935449\n"
     ]
    }
   ],
   "source": [
    "trn_loss = []\n",
    "net.train()\n",
    "\n",
    "for e  in range(100):\n",
    "    avg_loss = 0\n",
    "    for i, (x, t) in enumerate(cifar_loader):\n",
    "        starter.record()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = x.cuda()\n",
    "        t = t.cuda()\n",
    "        \n",
    "        y = net(x)\n",
    "\n",
    "        loss = criterion(y, t)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        trn_loss.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        ender.record()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        e_time = starter.elapsed_time(ender)\n",
    "        \n",
    "        writer.add_scalar('batch time', e_time, e * len(cifar_loader) + i)\n",
    "        writer.add_scalar('training loss', loss.item(), e * len(cifar_loader) + i)\n",
    "\n",
    "    avg_loss = avg_loss / len(cifar_trn_loader)\n",
    "    print('[Epoch %02i] %.8f' % (e, avg_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc6ce340-d509-405b-a983-c2922a0c6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'resnet_100epochs_20220420.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fac062-202a-49ab-be2a-d349a7355b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
